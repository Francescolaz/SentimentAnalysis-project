{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "indonesian-carbon",
   "metadata": {},
   "source": [
    "# Model Training and Song Classification \n",
    "\n",
    "Functions used in the main code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-fleece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the tweets and Count occurrences of words in all the tweets\n",
    "\n",
    "def counter_wd(txt_tweets):\n",
    "    count = Counter()\n",
    "    for tweet in txt_tweets:\n",
    "        for wd in tweet.split(\" \"):\n",
    "            count[wd] += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-event",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the explanation of the predictions with LIME\n",
    "\n",
    "def lime_pred(texts):\n",
    "    _seq = tknizer.texts_to_sequences(texts)\n",
    "    _textdata = pad_sequences(_seq, maxlen=24, padding='post', truncating='post')\n",
    "    return model.predict(_textdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-assault",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Split the lyrics in sentences and clean them\n",
    "\n",
    "def sent_and_clean(txt):\n",
    "\n",
    "    str_punct = punctuation.replace(\"#\", \"\").replace(\"'\", \"\") + \"—\" + \"“\" + \"’\" + \"€\" + \"£\"\n",
    "    listof = []\n",
    "\n",
    "    txt_ascii = \" \".join(txt.encode(\"ascii\", errors=\"ignore\").decode().split())\n",
    "    sent_list = nltk.tokenize.sent_tokenize(txt_ascii)\n",
    "\n",
    "    for sent in sent_list:\n",
    "        sent_clean = sent.encode(\"ascii\", errors=\"ignore\").decode() # remove weird characters like '오늘 밤 지나 해가 뜰 때까지 계속'\n",
    "        sent_clean = sent_clean.lower().translate(str.maketrans(str_punct, ' ' * len(str_punct))) # remove punctuation\n",
    "        sent_clean = \" \".join([wd for wd in nltk.word_tokenize(sent_clean) if wd not in stopwords.words(\"english\")]) # remove stopwords and append\n",
    "        sent_clean = re.sub(r'\\d+', '', sent_clean) # remove digits\n",
    "        sent_clean = \" \".join([wd for wd in nltk.word_tokenize(sent_clean) if wd not in [\"'s\", \"n't\", \"'re\", \"'ll\", \"'m\", \"'\", \"'ve\", \"'d\", \"'cuz\"\n",
    "                                                                                         \"'til\", \"till\", \"i'mma\", \"'cause\", \"'em\", \"'ma\"]]) # remove abbreviations\n",
    "\n",
    "        if len(sent_clean.split()) > 2:\n",
    "            listof.append(sent_clean)\n",
    "\n",
    "    return listof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-tribune",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the padded sequences\n",
    "\n",
    "def sent_to_padseq(list_txt):\n",
    "\n",
    "    pads_list = []\n",
    "    if len(list_txt) > 1:\n",
    "        for lis in list_txt:\n",
    "            if len(lis.split()) <= 24:\n",
    "                sent_seq = tknizer.texts_to_sequences([lis])\n",
    "                sent_pad = pad_sequences(sent_seq, maxlen=24, padding=\"post\", truncating=\"post\").tolist()[0]\n",
    "                pads_list.append(sent_pad)\n",
    "\n",
    "    return pads_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-sodium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Ragged Tensor to store the predictions of the model\n",
    "\n",
    "def pred_tensor(pad_column):\n",
    "    list_arrays = []\n",
    "\n",
    "    for elem in pad_column:\n",
    "        pred = model.predict(elem)\n",
    "        list_arrays.append(pred)\n",
    "\n",
    "    rag_tensor = tf.ragged.constant(list_arrays)\n",
    "    return rag_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-union",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the max by columns for each row, representing the prevalent emotion for each sentence in a song\n",
    "\n",
    "def get_maxEmotion(prob_tensor):\n",
    "    list_max = []\n",
    "\n",
    "    for array in prob_tensor:\n",
    "        np_array = array.numpy()\n",
    "        max_idx = np.argmax(np_array, axis=1)\n",
    "        if max_idx.shape[0] > 1:\n",
    "            list_max.append([[x] for x in np.transpose(max_idx).tolist()])\n",
    "        else:\n",
    "            list_max.append([max_idx])\n",
    "\n",
    "    max_tensor = tf.ragged.constant(list_max)\n",
    "    return max_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequencies of emotions with respect to each song at their corresponding index position \n",
    "\n",
    "def get_emotefreq(max_tensor):\n",
    "    list_freq = []\n",
    "\n",
    "    for array in max_tensor:\n",
    "        default_freq = [0, 0, 0, 0]\n",
    "\n",
    "        np_array = array.numpy()\n",
    "        array_freq = np.unique(np_array, return_counts=True)\n",
    "\n",
    "        if array_freq[0].shape[0] < 4: # If not all the emotions are included, then set their frequencies to zero\n",
    "            valfreq_tup = list(zip(array_freq[0], array_freq[1]))\n",
    "            for val, freq in valfreq_tup:\n",
    "                default_freq[val] = freq\n",
    "            list_freq.append(default_freq)\n",
    "\n",
    "        else:\n",
    "            list_freq.append(array_freq[1].tolist())\n",
    "\n",
    "    freq_numpy = np.array(list_freq)\n",
    "    return freq_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-burke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heuristic rule suitable for multi-label classification of the songs\n",
    "\n",
    "def classify_emote(emote_counts, perc_rule):\n",
    "    list_class = []\n",
    "    class_voc = dict([(0, 'anger'), (1, 'fear'), (2, 'joy'), (3, 'sadness')])\n",
    "\n",
    "    for song in emote_counts:\n",
    "        satisfy_idx = [idx for idx, val in enumerate(song.tolist()) if val/np.sum(song) >= perc_rule]\n",
    "        satisfy_class = [class_voc[idx] for idx in satisfy_idx]\n",
    "        list_class.append(satisfy_class)\n",
    "\n",
    "    #class_tensor = tf.ragged.constant(list_class, dtype=tf.dtypes.string)\n",
    "    return pd.Series(list_class, name='Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-edwards",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distinct Combination of genres and emotions\n",
    "\n",
    "def unique_tup(list1):\n",
    "    unique_list = []\n",
    "    for x in list1:\n",
    "        if x not in unique_list:\n",
    "            unique_list.append(x)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return unique_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Text_Mining",
   "language": "python",
   "name": "text_mining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
