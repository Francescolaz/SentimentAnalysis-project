{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "electronic-police",
   "metadata": {},
   "source": [
    "# Text Pre-processing\n",
    "\n",
    "Functions used in the main code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-ticket",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Cleaning\n",
    "\n",
    "def clean_up(dataf, text, str_punct, remove_stopwd=False, remove_emoji=True):\n",
    "    clean_dataf = dataf.copy()\n",
    "    tk = TweetTokenizer()\n",
    "    stopwd = stopwords.words(\"english\")\n",
    "    clean_txt = []\n",
    "\n",
    "    if text == \"Tweet\":\n",
    "        text_list = clean_dataf[\"Tweet\"].tolist()\n",
    "    elif text == \"Lyrics\":\n",
    "        text_list = clean_dataf[\"Lyrics\"].tolist()\n",
    "\n",
    "    for txt in text_list:\n",
    "        if type(txt) == str:\n",
    "            txt_free = re.sub(r'\\d+', '', txt.lower())  # remove digits\n",
    "            if text == \"Tweet\":\n",
    "                txt_free = re.sub(\"@[A-Za-z0-9]+\", \"\", txt_free)  # remove mentions \"@user\"\n",
    "            txt_free = txt_free.translate(\n",
    "                str.maketrans(str_punct, ' ' * len(str_punct)))  # remove punctuation and add blank space\n",
    "            if remove_emoji:\n",
    "                txt_free = re.sub(r\":[^>]+:\", \"\", emoji.demojize(txt_free))  # remove emoji \":emoji_code:\"\n",
    "            if remove_stopwd:\n",
    "                clean_txt.append(\" \".join([wd for wd in tk.tokenize(txt_free) if wd not in stopwd]))  # remove stopwords\n",
    "            elif not remove_stopwd:\n",
    "                clean_txt.append(\" \".join(tk.tokenize(txt_free)))\n",
    "        elif type(txt) != str and text == \"Tweet\":\n",
    "            clean_txt.append(txt)\n",
    "\n",
    "    clean_dataf[text] = clean_txt\n",
    "    return clean_dataf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-shopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Lemmatization, Tokenization and Part of Speech (POS) tagging\n",
    "\n",
    "def tk_POStag(clean_dataf, lemmatize=True):\n",
    "    tk = TweetTokenizer()\n",
    "    tk_dataf = clean_dataf.copy()\n",
    "    text_list = clean_dataf[\"Tweet\"].tolist()\n",
    "\n",
    "    if lemmatize:\n",
    "        lemmtzr = WordNetLemmatizer()\n",
    "        lemm_text = []\n",
    "        for txt in text_list:\n",
    "            lemm_text.append(\" \".join([lemmtzr.lemmatize(wd) for wd in txt.split()]))\n",
    "        tk_dataf[\"Tweet\"] = lemm_text\n",
    "\n",
    "    tk_dataf[\"Tokens POS\"] = [pos_tag(tk.tokenize(txt)) for txt in\n",
    "                              tk_dataf[\"Tweet\"]]  # additional column with list of POS tags\n",
    "\n",
    "    return tk_dataf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Text_Mining",
   "language": "python",
   "name": "text_mining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
